# ğŸ¦™ Ollama - DocumentaciÃ³n

GuÃ­a de integraciÃ³n de **Ollama** como proveedor de IA local en AutoNotiSocial.

## ğŸ“– Â¿QuÃ© es Ollama?

Ollama es una herramienta que permite ejecutar modelos de lenguaje grandes (LLMs) de forma local. En AutoNotiSocial, se usa para:

- **Generar resÃºmenes** de artÃ­culos de noticias
- **Crear contenido** para publicaciones en redes sociales
- **Procesar texto** sin depender de APIs externas

### Ventajas de Ollama

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| ğŸ”’ **Privacidad** | Los datos nunca salen de tu servidor |
| ğŸ’° **Sin costos** | No hay cargos por uso de API |
| âš¡ **Sin lÃ­mites** | Sin rate limits ni restricciones |
| ğŸŒ **Offline** | Funciona sin conexiÃ³n a internet |

---

## ğŸš€ Modelos Disponibles

### Modelos Recomendados

| Modelo | TamaÃ±o | RAM MÃ­nima | DescripciÃ³n |
|--------|--------|------------|-------------|
| `llama3.2` | 2GB | 4GB | Mejor balance calidad/velocidad |
| `llama3.2:1b` | 1.3GB | 2GB | MÃ¡s rÃ¡pido, ideal para servidores pequeÃ±os |
| `mistral` | 4GB | 8GB | Excelente para resÃºmenes |
| `phi3` | 2.3GB | 4GB | Microsoft, muy preciso |
| `gemma2` | 5GB | 8GB | Google, multilingÃ¼e |
| `qwen2` | 4GB | 8GB | Alibaba, bueno en espaÃ±ol |

### Cambiar Modelo

```bash
# VÃ­a variable de entorno
OLLAMA_MODEL=mistral docker-compose up -d

# O editar .env
echo "OLLAMA_MODEL=mistral" >> .env
echo "OLLAMA_PULL_MODEL=mistral" >> .env
docker-compose up -d --build
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

```env
# Proveedor de IA
AI_PROVIDER=ollama

# URL del servidor Ollama (dentro del contenedor)
OLLAMA_URL=http://127.0.0.1:11434

# Modelo a usar para generar resÃºmenes
OLLAMA_MODEL=llama3.2

# Modelo a descargar automÃ¡ticamente al iniciar
OLLAMA_PULL_MODEL=llama3.2
```

### ConfiguraciÃ³n del Backend

El servicio de IA se configura automÃ¡ticamente en `backend/src/services/ai/`:

```javascript
// InicializaciÃ³n
const provider = process.env.AI_PROVIDER || 'ollama';
const model = process.env.OLLAMA_MODEL || 'llama3.2';
const url = process.env.OLLAMA_URL || 'http://127.0.0.1:11434';
```

---

## ğŸ”§ Comandos Ãštiles

### GestiÃ³n de Modelos

```bash
# Listar modelos instalados
docker-compose exec autonotisocial ollama list

# Descargar nuevo modelo
docker-compose exec autonotisocial ollama pull mistral

# Eliminar modelo (liberar espacio)
docker-compose exec autonotisocial ollama rm llama3.2

# Ver informaciÃ³n del modelo
docker-compose exec autonotisocial ollama show llama3.2
```

### Probar Modelo

```bash
# Prueba interactiva
docker-compose exec autonotisocial ollama run llama3.2

# Prueba con prompt especÃ­fico
docker-compose exec autonotisocial ollama run llama3.2 "Resume en 3 lÃ­neas: La inteligencia artificial estÃ¡ transformando la industria tecnolÃ³gica..."
```

### API de Ollama

```bash
# Verificar que Ollama estÃ¡ corriendo
curl http://localhost:11434/api/tags

# Generar texto
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Hola, Â¿cÃ³mo estÃ¡s?",
  "stream": false
}'

# Chat
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {"role": "user", "content": "Resume este artÃ­culo: ..."}
  ],
  "stream": false
}'
```

---

## ğŸ“Š Uso en el Sistema

### Flujo de Resumen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArtÃ­culo  â”‚â”€â”€â”€â”€â–ºâ”‚   Backend    â”‚â”€â”€â”€â”€â–ºâ”‚   Ollama    â”‚
â”‚  (Scrapeado)â”‚     â”‚  (ai.js)     â”‚     â”‚  (LLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                    â”‚
                           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚     Resumen
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    SQLite    â”‚
                    â”‚  (summaries) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt de Resumen

El sistema usa un prompt optimizado para noticias tecnolÃ³gicas:

```
Eres un periodista tecnolÃ³gico experto. Resume el siguiente artÃ­culo 
en un formato adecuado para redes sociales.

Requisitos:
- MÃ¡ximo 280 caracteres (para Twitter)
- Incluye los puntos clave
- Tono profesional pero accesible
- En espaÃ±ol

ArtÃ­culo:
{contenido_del_articulo}
```

---

## ğŸ›ï¸ OptimizaciÃ³n de Rendimiento

### ParÃ¡metros de GeneraciÃ³n

```javascript
const options = {
    temperature: 0.7,      // Creatividad (0-1)
    top_p: 0.9,           // Nucleus sampling
    top_k: 40,            // Top-k sampling
    num_predict: 150,     // MÃ¡ximo de tokens
    repeat_penalty: 1.1   // PenalizaciÃ³n de repeticiÃ³n
};
```

### Ajustes por Caso de Uso

| Caso de Uso | Temperature | Tokens |
|-------------|-------------|--------|
| ResÃºmenes precisos | 0.3 | 100 |
| Contenido creativo | 0.8 | 200 |
| Traducciones | 0.1 | 300 |

---

## ğŸ’» Requisitos de Hardware

### MÃ­nimos

| Componente | Requisito |
|------------|-----------|
| RAM | 4GB |
| CPU | 4 cores |
| Disco | 10GB libres |
| Modelo | `llama3.2:1b` |

### Recomendados

| Componente | Requisito |
|------------|-----------|
| RAM | 8GB+ |
| CPU | 8 cores |
| GPU | NVIDIA con 4GB+ VRAM (opcional) |
| Disco | 20GB+ SSD |
| Modelo | `llama3.2` o `mistral` |

### Con GPU (AceleraciÃ³n)

Para usar GPU NVIDIA dentro de Docker:

```yaml
# docker-compose.yml
services:
  autonotisocial:
    # ... otras configuraciones ...
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

---

## ğŸ”„ Ollama vs Gemini

| CaracterÃ­stica | Ollama | Gemini |
|----------------|--------|--------|
| Costo | Gratis | Pago por uso |
| Privacidad | 100% local | Datos en la nube |
| Velocidad | Depende del hardware | Muy rÃ¡pido |
| Calidad | Buena-Excelente | Excelente |
| Offline | âœ… SÃ­ | âŒ No |
| ConfiguraciÃ³n | Incluido en Docker | Solo API Key |

### Cambiar entre Proveedores

```bash
# Usar Gemini
AI_PROVIDER=gemini GEMINI_API_KEY=tu_key docker-compose up -d

# Usar Ollama
AI_PROVIDER=ollama docker-compose up -d
```

---

## ğŸ› Troubleshooting

### Ollama no responde

```bash
# Verificar estado
docker-compose exec autonotisocial supervisorctl status ollama

# Reiniciar Ollama
docker-compose exec autonotisocial supervisorctl restart ollama

# Ver logs
docker-compose exec autonotisocial supervisorctl tail -f ollama
```

### Modelo no se descarga

```bash
# Descargar manualmente
docker-compose exec autonotisocial ollama pull llama3.2

# Verificar espacio en disco
docker-compose exec autonotisocial df -h
```

### Respuestas lentas

1. **Usar modelo mÃ¡s pequeÃ±o**: `llama3.2:1b`
2. **Aumentar memoria del contenedor** en `docker-compose.yml`
3. **Habilitar GPU** si estÃ¡ disponible

### Error "out of memory"

```bash
# Cambiar a modelo mÃ¡s pequeÃ±o
OLLAMA_MODEL=llama3.2:1b docker-compose up -d

# O aumentar lÃ­mite de memoria
# En docker-compose.yml:
# deploy.resources.limits.memory: 16G
```

---

## ğŸ“š Recursos

- [Ollama Official](https://ollama.com/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Modelos Disponibles](https://ollama.com/library)
- [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
